models: AlexNet, GoogLeNet, ResNet, MobilenetV2, MNASNet, SqueezeNet, ShuffleNetV2, DenseNet161, VGG16, and InceptionV3
dataset: ImageNet
input_shape:

Tiny-ImageNet：http://cs231n.stanford.edu/tiny-imagenet-200.zip
    https://www.cnblogs.com/liuyangcode/p/14689893.html


''' import models (pre-trained by ImageNet):
import torchvision.models as models

resnet18 = models.resnet18(pretrained=True)
alexnet = models.alexnet(pretrained=True)
squeezenet = models.squeezenet1_0(pretrained=True)
vgg16 = models.vgg16(pretrained=True)
densenet = models.densenet161(pretrained=True)
inception = models.inception_v3(pretrained=True)
googlenet = models.googlenet(pretrained=True)
shufflenet = models.shufflenet_v2_x1_0(pretrained=True)
mobilenet = models.mobilenet_v2(pretrained=True)
resnext50_32x4d = models.resnext50_32x4d(pretrained=True)
wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)
mnasnet = models.mnasnet1_0(pretrained=True)

'''



''' from torchvision.test_model.py
# The following contains configuration and expected values to be used tests that are model specific
_model_tests_values = {
    "retinanet_resnet50_fpn": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [36, 46, 65, 78, 88, 89],
    },
    "retinanet_resnet50_fpn_v2": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [44, 74, 131, 170, 200, 203],
    },
    "keypointrcnn_resnet50_fpn": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [48, 58, 77, 90, 100, 101],
    },
    "fasterrcnn_resnet50_fpn": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [30, 40, 59, 72, 82, 83],
    },
    "fasterrcnn_resnet50_fpn_v2": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [50, 80, 137, 176, 206, 209],
    },
    "maskrcnn_resnet50_fpn": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [42, 52, 71, 84, 94, 95],
    },
    "maskrcnn_resnet50_fpn_v2": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [66, 96, 153, 192, 222, 225],
    },
    "fasterrcnn_mobilenet_v3_large_fpn": {
        "max_trainable": 6,
        "n_trn_params_per_layer": [22, 23, 44, 70, 91, 97, 100],
    },
    "fasterrcnn_mobilenet_v3_large_320_fpn": {
        "max_trainable": 6,
        "n_trn_params_per_layer": [22, 23, 44, 70, 91, 97, 100],
    },
    "ssd300_vgg16": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [45, 51, 57, 63, 67, 71],
    },
    "ssdlite320_mobilenet_v3_large": {
        "max_trainable": 6,
        "n_trn_params_per_layer": [96, 99, 138, 200, 239, 257, 266],
    },
    "fcos_resnet50_fpn": {
        "max_trainable": 5,
        "n_trn_params_per_layer": [54, 64, 83, 96, 106, 107],
    },
}
'''

##Summary of models and dataset in Mobile AI
 - in https://ai-benchmark.com/tests.html

Recognition task:
- MobileNet - V2, 224x224, Imagenet + MS COCO
    train Imagenet, detection: MS COCO (with a modified version of the Single Shot Detector (SSD))
- Inception -V3, 346x346, ILSVRC 2012, inference: recognition
- MobileNet - V3,  512 x 512,
    classification: Imagenet, MobilenetNet V3
    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)
    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head
- EfficientNet-B4, 380 x 380 ,  CIFAR-10, subset of the CamSDD datase
- Inception -V3, 346x346, NA

Object Detection:
 - YOLOv4-Tiny, 416 x 416,
    dataset： train with imagenet, detect with: MS COCO

 - CRNN / Bi-LSTM , 64 x 200
    train: synthetic dataset (Synth) released by Jaderberg et al. [20]
    test: ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).

Semantic Segmentation:
 -  DeepLab-V3+ , 1024 x 1024,
    dataset:
        employ ImageNet-1k [74] pretrained ResNet-101 [25] or modified aligned Xception [26,31] to extract dense feature maps
        test: PASCAL VOC 2012 and Cityscapes datasets
            backbone: {ResNet-101:513 x 513, xception: 299 x 299}

Super resolution:
 - ESRGAN, 512 x512,
    dataset:
        train: DIV2K dataset (PIRM2018-SR Challenge), Flickr2K,  OutdoorSceneTraining (OST)
        evaluate: Set5 [42], Set14 [43], BSD100 [44], Urban100
 - SRGAN, 1024 x 1024
    dataset:
        train: a random sample of 350 thousand images from the ImageNet database
        evaluate: Set5 [3], Set14 [69] and BSD100

Video Super-Resolution:
 - XLSR, 1080 x 1920
    dataset:
        train:Div2K
        evaluate: Div2K 100 test images,  Set5, Set14, BSD100, Manga109, Urban100
 - VSR, 2160 x 3840, dataset:  {train and test: DIV2K}

