{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef62efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--root root] [--split split]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/royliu/.local/share/jupyter/runtime/kernel-7272269b-491a-48c2-9f3e-bc8e25207f25.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. download zip files via:\n",
    " https://www.cityscapes-dataset.com/dataset-overview/\n",
    " gtFine_trainvaltest.zip (241MB) [md5], leftImg8bit_trainvaltest.zip (11GB) [md5]\n",
    "\n",
    "2. manually unzip or run codes to unzip files, as:\n",
    "\n",
    "from torchvision.datasets import Cityscapes as cityscapes  # will unzip the file when run at first time\n",
    "import os\n",
    "root ='/home/royliu/Documents/datasets/'\n",
    "data_dir ='cityscapes'\n",
    "data_dir = os.path.join(root, data_dir)\n",
    "dataset = cityscapes(data_dir, split = 'train', mode ='fine', target_type = 'semantic')\n",
    "\n",
    "will get data in below structure\n",
    "\n",
    "├── gtFine\n",
    "│   ├── train\n",
    "│   │   ├── aachen\n",
    "│   │   ├── bochum\n",
    "│   │   └── ......\n",
    "│   └── val\n",
    "│       └── frankfurt\n",
    "└── leftImg8bit\n",
    "    ├── train\n",
    "    │   ├── aachen\n",
    "    │   ├── bochum\n",
    "    │   └── ......\n",
    "    └── val\n",
    "        └── frankfurt\n",
    "\n",
    "\n",
    "output:  4 dimentions ndarray of: mask, image\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root', metavar='root', default='/home/royliu/Documents/datasets/cityscapes')\n",
    "parser.add_argument('--split', metavar='split', default='eval', help='\"train\" or \"eval\".')\n",
    "\n",
    "def cv_to_pil(img_cv): # convert cv2 to PIL format\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "    # img_pil = Image.fromarray(img_cv)  # not convert RGB sequence\n",
    "    return img_pil\n",
    "\n",
    "\n",
    "class CityTransform:\n",
    "    def __call__(self, image, mask):\n",
    "        image = cv_to_pil(image)  # to satisefy transforms.RandomResizedCrop's input requirement in shape of (...,h,w)\n",
    "        transform_img = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop((256, 512)),\n",
    "                                # transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        transform_mask = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop((256, 512))])\n",
    "        return transform_img(image), transform_mask(mask)\n",
    "\n",
    "class DataGenerator(Cityscapes):  ## varible names in Cityscapes Class are: images, targets, split...\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs, target_type=\"semantic\")\n",
    "        self.semantic_target_type_index = [i for i, t in enumerate(self.target_type) if t == \"semantic\"][0]\n",
    "        self.colormap = self._generate_colormap()\n",
    "        # self.transform = self._transform()\n",
    "\n",
    "    def _transform(self, image, mask):\n",
    "        transform = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop((256, 512)),\n",
    "                                # transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        return transform(image), transform(mask)\n",
    "\n",
    "\n",
    "    def _generate_colormap(self):\n",
    "        colormap = {}\n",
    "        for class_ in self.classes:\n",
    "            if class_.train_id in (-1, 255):\n",
    "                continue\n",
    "            colormap[class_.train_id] = class_.id\n",
    "        return colormap  # return a diction\n",
    "\n",
    "    def _convert_to_segmentation_mask(self, mask):\n",
    "        height, width = mask.shape[:2]\n",
    "        segmentation_mask = np.zeros((height, width, len(self.colormap)), dtype=np.float32)\n",
    "        for label_index, label in self.colormap.items():\n",
    "            segmentation_mask[:, :, label_index] = (mask == label).astype(float)\n",
    "        return segmentation_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index])\n",
    "        # image = cv_to_pil(image)  # transfer shape to channel, h, w\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # still in cv2 format\n",
    "        image = image.transpose(2,0,1)  # now in shape of channel,h,w & RGB\n",
    "\n",
    "        mask = cv2.imread(self.targets[index][self.semantic_target_type_index], cv2.IMREAD_UNCHANGED)\n",
    "        mask = self._convert_to_segmentation_mask(mask)\n",
    "        mask = mask.transpose(2, 0, 1)  # transfer shape to class#, h, w\n",
    "\n",
    "        # if self.transform :\n",
    "        #     transformed = self._transform(image=image, mask=mask)\n",
    "        #     image = transformed[\"image\"]\n",
    "        #     mask = transformed[\"mask\"]\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "\n",
    "# cv2.setNumThreads(0)\n",
    "# cv2.ocl.setUseOpenCL(False)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop((256, 512)),\n",
    "#     # transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "args = parser.parse_args()\n",
    "# assert os.path.exists(args.root), 'Root of dataset is incorrect or miss.'\n",
    "# dataset_train = DataGenerator(args.root, split = 'val', transforms= CityTransform) # default: mode='fine', target_type= 'sementic, split: train, test or val if mode=”fine” otherwise train, train_extra or val\n",
    "# img_array, sgm = dataset_train[0]\n",
    "# print(img_array.shape, sgm.shape)\n",
    "# img_pil = Image.fromarray(img_array, 'RGB')\n",
    "# img_pil.show()\n",
    "# print(img.shape, sgm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
