{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b4d098",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- alexnet\n",
    "- vgg\n",
    "- resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv5-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "## JSON横竖切换\n",
    "  - 快捷键Ctrl + Alt + L "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbc11d",
   "metadata": {},
   "source": [
    "### Dataset summary:\n",
    "    - Recognition: Imagenet(ILSVRC 2012), CIFAR-10\n",
    "    - Detect: COCO, Imagenet(ILSVRC 2012)\n",
    "    - Semantic Segmentation: Cityscapes, PASCAL VOC 2012, R-ASPP and Lite R-ASPP\n",
    "    - Super Resolution: DIV2k, Set5, Set14, BSD100, Urban100, Manga109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e39283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a820878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check models \n",
    "import torchvision.models as models\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e91db25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Cityscapes\n",
       "    Number of datapoints: 2975\n",
       "    Root location: /home/royliu/Documents/datasets/cityscapes\n",
       "    Split: train\n",
       "    Mode: gtFine\n",
       "    Type: ['semantic']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cityscapes dataset\n",
    "from torchvision.datasets import Cityscapes as cityscapes  # will unzip the file when run at first time\n",
    "import os\n",
    "root ='/home/royliu/Documents/datasets/'\n",
    "data_dir ='cityscapes'\n",
    "\n",
    "data_dir = os.path.join(root, data_dir)\n",
    "dataset = cityscapes(data_dir, split = 'train', mode ='fine', target_type = 'semantic')\n",
    "img, smnt = dataset[0]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS coco detection dataset\n",
    "CLASS torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'deeplabv3_resnet50':'Cityscapes', }\n",
    "\n",
    "{'deeplabv3_resnet50':'torchvision.models.segmentation.deeplabv3_resnet50'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://pytorch.org/vision/stable/models.html \n",
    "'''\n",
    "model_list_all = ['alexnet', 'convnext_base', 'densenet121','densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mnasnet0_5','mobilenet_v2','mobilenet_v3_small',\\\n",
    "                  'regnet_y_400mf','resnet18','resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                  'squeezenet1_0', 'squeezenet1_1','vgg11', 'vgg16', 'vgg19','vit_b_16']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1c3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "# args_train, args_infer = [] ,[]\n",
    "# config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# # training\n",
    "# img_size_list = [224]\n",
    "# batch_size_list_train = [64,128,256,512,1024]\n",
    "# train_config_list = config_list\n",
    "# for image_size in img_size_list:\n",
    "#     for model in train_config_list:\n",
    "#         for batch_size in batch_size_list_train:\n",
    "#             new_args = {'arch': model,'workers': 1, 'epochs': 5, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "#             args_train.append(new_args)\n",
    "\n",
    "# # infering   \n",
    "# img_size_list = [224]\n",
    "# batch_size_list_infer = [1]\n",
    "# infer_config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "# for image_size in img_size_list:\n",
    "#     for model in infer_config_list:\n",
    "#         for batch_size in batch_size_list_infer:\n",
    "#             new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "#             args_infer.append(new_args)\n",
    "        \n",
    "# tr = json.dumps(args_train)\n",
    "# inf= json.dumps(args_infer)\n",
    "# inf\n",
    "# tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a757e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "532020c6",
   "metadata": {},
   "source": [
    "# Experiments record:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667fbc5",
   "metadata": {},
   "source": [
    "\n",
    "### by 12_21_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [64, 128,256,512]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "\n",
    "\n",
    "### by 12_22_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1024]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c818f",
   "metadata": {},
   "source": [
    "##  01172023\n",
    "### training:\n",
    "  - deeplab_v3'\n",
    "       batch size: 2,4,6,8\n",
    "  - 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "       batch size: 64,128,256,512,1024\n",
    "\n",
    "### Inference:\n",
    " - yolov5\n",
    " - 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24d3b2",
   "metadata": {},
   "source": [
    "##  COMBINATION 01/20/2023 - 01/23a/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    - 'yolo_v5', 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "    \n",
    "  #### except:\n",
    "    train:\n",
    "    - 'shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "    \n",
    "    infer:\n",
    "    'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab169a",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ec7b0",
   "metadata": {},
   "source": [
    "## COMBINATION: 01/23/2023 - 01/24/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    -  'yolo_v5s', 'alexnet','densenet201','mobilenet_v2'\n",
    "    - batch size: 1\n",
    "    - image size: 448\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1032d2",
   "metadata": {},
   "source": [
    "###  \\# 01/25/2023 - 01/26a/2023\n",
    "#### COMBINATION: \n",
    "#### training:\n",
    "- 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "- batch size: 2,4,8,16,32,64,128,256,512,1024\n",
    "- image size 224\n",
    "\n",
    "#### Inference:\n",
    "-  'yolo_v5s', 'alexnet'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "###  \\# 01/27/2023 - 01/28/2023, for training duration comparing individually training and co-work with inference\n",
    "#### COMBINATION: \n",
    "#### Training: \n",
    "- 'deeplab_v3': batch size:  8\n",
    "- 'vgg16','resnet152': batch size: 64 and 32 individually\n",
    "- Epochs: 5, image size 224\n",
    "\n",
    "#### Inference: \n",
    "- 'None', 'yolo_v5s','mobilenet_v2'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "#### result: canceled with exception (after runing 43992 seconds)\n",
    "##### Exception:\n",
    "Train arch_vgg16 workers_1 epochs_5 batch_size_64 image_size_224 device_cuda + Infer arch_None workers_1 batch_size_1 image_size_224 device_cuda\n",
    "{'data': '/home/royliu/Documents/datasets', 'dataset': 'imagenet', 'arch': 'vgg16', 'workers': 1, 'epochs': 5, 'start_epoch': 0, 'batch_size': 64, 'lr': 0.001, 'momentum': 0.9,\n",
    "\n",
    "100%|██████████| 20019/20019 [13:03:14<00:00,  2.35s/it]\n",
    " 43992  instances. *   Acc@1 0.918 Acc@5 3.806\n",
    " FileNotFoundError: [Errno 2] No such file or directory: 'checkpoint.pth.tar'\n",
    " \n",
    "### \\# 01/29/2023 - 02/03/2023, for training with/wo training co-working\n",
    "#### COMBINATION: \n",
    "##### Training: \n",
    "- 'vgg16','resnet152': batch size: 64 and 32 individually\n",
    "- Epochs: 5, image size 224\n",
    "\n",
    "##### Inference: \n",
    "- 'yolo_v5s','None', 'mobilenet_v2'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "### # 02/1/2023 - 02/2/2023,  for training with/wo inference co-working\n",
    "#### COMBINATION: \n",
    "##### Training: \n",
    "- 'deeplab_v3': batch size: 8\n",
    "- Epochs: 5, image size 224\n",
    "\n",
    "##### Inference: \n",
    "- 'yolo_v5s','None', 'mobilenet_v2'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "\n",
    "### 02/01/2023 - 02/09/2023, for inference with/wo training co-working\n",
    "##### Training: \n",
    "- 'vgg16','resnet152': batch size: 64 and 32 individually\n",
    "    - Epochs: -, image size 224\n",
    "\n",
    "##### Inference: \n",
    "- 'yolov5s'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "- None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f7e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec43a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b4a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 29 models.\n",
      "Totally 24 models.\n",
      "Totally 21 models.\n"
     ]
    }
   ],
   "source": [
    "## calculate combinations for inference latency predict experiments - train+infer, 20230220\n",
    "\n",
    "## 1, too many, not use\n",
    "cnn_model_list1 = ['alexnet', 'convnext_base', 'densenet121','densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mnasnet0_5','mobilenet_v2','mobilenet_v3_small',\\\n",
    "                  'regnet_y_400mf','resnet18','resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                  'squeezenet1_0', 'squeezenet1_1','vgg11', 'vgg16', 'vgg19','vit_b_16']\n",
    "\n",
    "\n",
    "yolo_model_list = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "deeplab_model_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "model_list = cnn_model_list1 + yolo_model_list + deeplab_model_list\n",
    "print(f'Totally {len(model_list)} models.')\n",
    "\n",
    "bz_list_cnn = [16,32,64,128,256,512,1024] #21*7=147\n",
    "bz_list_deeplab = [1,2,4,8] #3*4 =12\n",
    "\n",
    "# Total1 159 training combinations\n",
    "# total1 29 infer combinations\n",
    "\n",
    "\n",
    "## 2, we use this\n",
    "cnn_model_list2 = ['alexnet', 'convnext_base', 'densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mnasnet0_5','mobilenet_v2',\\\n",
    "                  'regnet_y_400mf','resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                   'squeezenet1_1', 'vgg16', 'vgg19','vit_b_16']\n",
    "\n",
    "\n",
    "\n",
    "deeplab_model_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "model_list = cnn_model_list2 + yolo_model_list + deeplab_model_list\n",
    "print(f'Totally {len(model_list)} models.')\n",
    "\n",
    "# for 2: to complete\n",
    "# Total1 124 training combinations\n",
    "# total1 24 infer combinations\n",
    "\n",
    "\n",
    "\n",
    "# for 3: to seperate\n",
    "cnn_model_list3 = ['alexnet', 'convnext_base', 'densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mobilenet_v2',\\\n",
    "                 'resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                   'squeezenet1_1', 'vgg16', 'vgg19']  # 13 of 18 models\n",
    "yolo_model_list = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "deeplab_model_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "model_list3 = cnn_model_list3 + yolo_model_list + deeplab_model_list\n",
    "print(f'Totally {len(model_list3)} models.')\n",
    "\n",
    "bz_list_cnn = [16,32,64,128,256,512,1024] # 13*7=91\n",
    "bz_list_deeplab = [4,8] #6\n",
    "# for 2: to seperate\n",
    "# Total1 124 training combinations\n",
    "# total1 24 infer combinations\n",
    "\n",
    "# train models:97, infer models: 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608c577",
   "metadata": {},
   "source": [
    "# Recorded in shared folder\n",
    "### 02/10/2023 - 02/12/2023, for inference with/wo training co-working,   44 combinations\n",
    "##### Training: \n",
    "- 'vgg16','resnet152','alexnet': batch size: 64, 32, and 1024 individually\n",
    "- 'deeplab_v3': batch size: 8 \n",
    "    - Epochs: 5, image size 224\n",
    "- 'none'\n",
    "\n",
    "##### Inference: \n",
    "-     arch_list = ['yolov5s','alexnet',  'densenet121',  'efficientnet_v2_l', 'googlenet', 'inception_v3',  'mobilenet_v3_small',  'resnet50', 'vgg16',  'deeplabv3_resnet50']\n",
    "    - batch size: 1\n",
    "    - image size: 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf90e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 21 models.\n"
     ]
    }
   ],
   "source": [
    "## 20/20/2303, for inference with training co-working\n",
    "#################################################### \n",
    "## train with cnn_model_list3+ deeplab_model_list ##\n",
    "## infer with model_list3#######\n",
    "####################################################\n",
    "\n",
    "cnn_model_list3 = ['alexnet', 'convnext_base', 'densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mobilenet_v2',\\\n",
    "                 'resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                   'squeezenet1_1', 'vgg16', 'vgg19']  # 13 of 18 models\n",
    "yolo_model_list = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "deeplab_model_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "model_list3 = cnn_model_list3 + yolo_model_list + deeplab_model_list\n",
    "print(f'Totally {len(model_list3)} models.')\n",
    "\n",
    "bz_list_cnn = [16,32,64,128,256,512,1024] # 13*7=91\n",
    "bz_list_deeplab = [4,8] #6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7567140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"deeplabv3_resnet50\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_resnet50\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_resnet101\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_resnet101\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_mobilenet_v3_large\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_mobilenet_v3_large\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train jason\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "config_list = ['vgg16', 'resnet152', 'alexnet','deeplabv3_resnet50']\n",
    "config_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "\n",
    "# training\n",
    "img_size_list = [224]\n",
    "batch_size_list_train = [4,8] ## deeplab_v3 is  2, 4, 6, 8 instead\n",
    "train_config_list = config_list\n",
    "for image_size in img_size_list:\n",
    "    for model in train_config_list:\n",
    "        for batch_size in batch_size_list_train:\n",
    "            new_args = {'arch': model,'workers': 1, 'epochs': 5, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_train.append(new_args)\n",
    "\n",
    "tr = json.dumps(args_train)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e64ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"convnext_base\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"efficientnet_v2_l\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"googlenet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"inception_v3\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet50\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_1\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg19\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolov5n\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolov5s\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolov5m\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolov5l\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolov5x\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_resnet50\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_resnet101\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplabv3_mobilenet_v3_large\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## infer jason\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "# config_list = ['yolo_v5', 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "infer_config_list = ['none','yolov5s','alexnet',  'densenet121',  'efficientnet_v2_l', \\\n",
    "                  'googlenet', 'inception_v3',  'mobilenet_v3_small',  'resnet50',  \\\n",
    "                                  'vgg16',  'deeplabv3_resnet50']\n",
    "cnn_model_list3 = ['alexnet', 'convnext_base', 'densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mobilenet_v2',\\\n",
    "                 'resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                   'squeezenet1_1', 'vgg16', 'vgg19']  # 13 of 18 models\n",
    "yolo_model_list = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "deeplab_model_list = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large']\n",
    "infer_config_list = cnn_model_list3 + yolo_model_list + deeplab_model_list\n",
    "\n",
    "# infering   \n",
    "img_size_list = [224]\n",
    "batch_size_list_infer = [1]\n",
    "\n",
    "for image_size in img_size_list:\n",
    "    for model in infer_config_list:\n",
    "        for batch_size in batch_size_list_infer:\n",
    "            new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_infer.append(new_args)\n",
    "            \n",
    "inf= json.dumps(args_infer)\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3820b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15258615",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7c6d5",
   "metadata": {},
   "source": [
    "#### models in torchvision.models:\n",
    " - inception_v3:\n",
    "    - RuntimeError: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
    "\n",
    " - googlenet:\n",
    "     - AttributeError: 'GoogLeNetOutputs' object has no attribute 'log_softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ad9a",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- MobileNet - V2, 224x224, Imagenet + MS COCO\n",
    "    train Imagenet, detection: MS COCO (with a modified version of the Single Shot Detector (SSD))\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- EfficientNet-B4, 380 x 380 ,  CIFAR-10, subset of the CamSDD datase\n",
    "- Inception -V3, 346x346, NA\n",
    "\n",
    "\n",
    " - alexnet\n",
    " - vgg\n",
    " - resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv4-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    " - CRNN / Bi-LSTM , 64 x 200\n",
    "    train: synthetic dataset (Synth) released by Jaderberg et al.\n",
    "    test: y ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "###### Super resolution:\n",
    " - ESRGAN, 512 x512,\n",
    "    dataset:\n",
    "        train: DIV2K dataset (PIRM2018-SR Challenge), Flickr2K,  OutdoorSceneTraining (OST)\n",
    "        evaluate: Set5 , Set14, BSD100, Urban100\n",
    " - SRGAN, 1024 x 1024\n",
    "    dataset:\n",
    "        train: a random sample of 350 thousand images from the ImageNet database\n",
    "        evaluate: Set5 , Set14 and BSD100\n",
    "\n",
    "###### Video Super-Resolution:\n",
    " - XLSR, 1080 x 1920\n",
    "    dataset:\n",
    "        train:Div2K\n",
    "        evaluate: Div2K 100 test images,  Set5, Set14, BSD100, Manga109, Urban100\n",
    " - VSR, 2160 x 3840, dataset:  {train and test: DIV2K}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3621fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'config.json'\n",
    "with open(file) as f:\n",
    "    config = json.load(f)\n",
    "args_train, args_infer= config['train'],  config['infer']\n",
    "len(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973aeb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'122415'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def date_time():\n",
    "    s_l = time.localtime(time.time())\n",
    "    date = time.strftime(\"%Y%m%d\", s_l)\n",
    "    tm = time.strftime(\"%H%M%S\", s_l)\n",
    "    # print(date, tm )\n",
    "    return date, tm\n",
    "\n",
    "date, tm = date_time()\n",
    "tm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
