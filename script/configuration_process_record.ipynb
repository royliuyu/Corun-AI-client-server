{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b7c6d5",
   "metadata": {},
   "source": [
    "## models in torchvision.models:\n",
    " - inception_v3:\n",
    "    - RuntimeError: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
    "\n",
    " - googlenet:\n",
    "     - AttributeError: 'GoogLeNetOutputs' object has no attribute 'log_softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d098",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- alexnet\n",
    "- vgg\n",
    "- resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv5-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "## JSON横竖切换\n",
    "  - 快捷键Ctrl + Alt + L "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbc11d",
   "metadata": {},
   "source": [
    "### Dataset summary:\n",
    "    - Recognition: Imagenet(ILSVRC 2012), CIFAR-10\n",
    "    - Detect: COCO, Imagenet(ILSVRC 2012)\n",
    "    - Semantic Segmentation: Cityscapes, PASCAL VOC 2012, R-ASPP and Lite R-ASPP\n",
    "    - Super Resolution: DIV2k, Set5, Set14, BSD100, Urban100, Manga109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e39283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a820878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check models \n",
    "import torchvision.models as models\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e91db25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Cityscapes\n",
       "    Number of datapoints: 2975\n",
       "    Root location: /home/royliu/Documents/datasets/cityscapes\n",
       "    Split: train\n",
       "    Mode: gtFine\n",
       "    Type: ['semantic']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cityscapes dataset\n",
    "from torchvision.datasets import Cityscapes as cityscapes  # will unzip the file when run at first time\n",
    "import os\n",
    "root ='/home/royliu/Documents/datasets/'\n",
    "data_dir ='cityscapes'\n",
    "\n",
    "data_dir = os.path.join(root, data_dir)\n",
    "dataset = cityscapes(data_dir, split = 'train', mode ='fine', target_type = 'semantic')\n",
    "img, smnt = dataset[0]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS coco detection dataset\n",
    "CLASS torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'deeplabv3_resnet50':'Cityscapes', }\n",
    "\n",
    "{'deeplabv3_resnet50':'torchvision.models.segmentation.deeplabv3_resnet50'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://pytorch.org/vision/stable/models.html \n",
    "'''\n",
    "model_list_all = ['alexnet', 'convnext_base', 'densenet121','densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mnasnet0_5','mobilenet_v2','mobilenet_v3_small',\\\n",
    "                  'regnet_y_400mf','resnet18','resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                  'squeezenet1_0', 'squeezenet1_1','vgg11', 'vgg16', 'vgg19','vit_b_16']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1c3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# training\n",
    "img_size_list = [224]\n",
    "batch_size_list_train = [64,128,256,512,1024]\n",
    "train_config_list = config_list\n",
    "for image_size in img_size_list:\n",
    "    for model in train_config_list:\n",
    "        for batch_size in batch_size_list_train:\n",
    "            new_args = {'arch': model,'workers': 1, 'epochs': 3, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_train.append(new_args)\n",
    "\n",
    "# infering   \n",
    "img_size_list = [224]\n",
    "batch_size_list_infer = [1]\n",
    "infer_config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "for image_size in img_size_list:\n",
    "    for model in infer_config_list:\n",
    "        for batch_size in batch_size_list_infer:\n",
    "            new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_infer.append(new_args)\n",
    "        \n",
    "tr = json.dumps(args_train)\n",
    "inf= json.dumps(args_infer)\n",
    "inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667fbc5",
   "metadata": {},
   "source": [
    "## Experiments record:\n",
    "### by 12_21_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [64, 128,256,512]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "\n",
    "\n",
    "### by 12_22_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1024]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c818f",
   "metadata": {},
   "source": [
    "##  01172023\n",
    "### training:\n",
    "  - deeplab_v3'\n",
    "       batch size: 2,4,6,8\n",
    "  - 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "       batch size: 64,128,256,512,1024\n",
    "\n",
    "### Inference:\n",
    " - yolov5\n",
    " - 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24d3b2",
   "metadata": {},
   "source": [
    "##  01/20/2023 - 01/23a/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    - 'yolo_v5s', 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "    \n",
    "  #### except:\n",
    "    train:\n",
    "    - 'shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "    \n",
    "    infer:\n",
    "    'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ec7b0",
   "metadata": {},
   "source": [
    "## new: 01/23/2023 - 01/24/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    -  'yolo_v5s', 'alexnet','densenet201','mobilenet_v2'\n",
    "    - batch size: 1\n",
    "    - image size: 448\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7567140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"deeplab_v3\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"shufflenet_v2_x1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"squeezenet1_0\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 2, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 4, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 8, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 16, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 32, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 128, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 256, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 512, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1024, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 3, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 20230118\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "config_list = ['deeplab_v3', 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# training\n",
    "img_size_list = [224]\n",
    "batch_size_list_train = [2,4,8,16,32,64,128,256,512,1024,1] ## deeplab_v3 is  2, 4, 6, 8 instead\n",
    "train_config_list = config_list\n",
    "for image_size in img_size_list:\n",
    "    for model in train_config_list:\n",
    "        for batch_size in batch_size_list_train:\n",
    "            new_args = {'arch': model,'workers': 1, 'epochs': 3, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_train.append(new_args)\n",
    "\n",
    "tr = json.dumps(args_train)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e64ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"yolo_v5s\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 448, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 448, \"device\": \"cuda\"}, {\"arch\": \"densenet201\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 448, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 448, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 20230118\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "# config_list = ['yolo_v5', 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# infering   \n",
    "img_size_list = [448]\n",
    "batch_size_list_infer = [1]\n",
    "infer_config_list = ['yolo_v5s','alexnet','densenet201','mobilenet_v2']\n",
    "for image_size in img_size_list:\n",
    "    for model in infer_config_list:\n",
    "        for batch_size in batch_size_list_infer:\n",
    "            new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_infer.append(new_args)\n",
    "            \n",
    "inf= json.dumps(args_infer)\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3820b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973aeb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'122415'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def date_time():\n",
    "    s_l = time.localtime(time.time())\n",
    "    date = time.strftime(\"%Y%m%d\", s_l)\n",
    "    tm = time.strftime(\"%H%M%S\", s_l)\n",
    "    # print(date, tm )\n",
    "    return date, tm\n",
    "\n",
    "date, tm = date_time()\n",
    "tm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15258615",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3621fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'config.json'\n",
    "with open(file) as f:\n",
    "    config = json.load(f)\n",
    "args_train, args_infer= config['train'],  config['infer']\n",
    "len(args_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ad9a",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- MobileNet - V2, 224x224, Imagenet + MS COCO\n",
    "    train Imagenet, detection: MS COCO (with a modified version of the Single Shot Detector (SSD))\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- EfficientNet-B4, 380 x 380 ,  CIFAR-10, subset of the CamSDD datase\n",
    "- Inception -V3, 346x346, NA\n",
    "\n",
    "\n",
    " - alexnet\n",
    " - vgg\n",
    " - resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv4-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    " - CRNN / Bi-LSTM , 64 x 200\n",
    "    train: synthetic dataset (Synth) released by Jaderberg et al.\n",
    "    test: y ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "###### Super resolution:\n",
    " - ESRGAN, 512 x512,\n",
    "    dataset:\n",
    "        train: DIV2K dataset (PIRM2018-SR Challenge), Flickr2K,  OutdoorSceneTraining (OST)\n",
    "        evaluate: Set5 , Set14, BSD100, Urban100\n",
    " - SRGAN, 1024 x 1024\n",
    "    dataset:\n",
    "        train: a random sample of 350 thousand images from the ImageNet database\n",
    "        evaluate: Set5 , Set14 and BSD100\n",
    "\n",
    "###### Video Super-Resolution:\n",
    " - XLSR, 1080 x 1920\n",
    "    dataset:\n",
    "        train:Div2K\n",
    "        evaluate: Div2K 100 test images,  Set5, Set14, BSD100, Manga109, Urban100\n",
    " - VSR, 2160 x 3840, dataset:  {train and test: DIV2K}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
