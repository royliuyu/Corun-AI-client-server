{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b4d098",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- alexnet\n",
    "- vgg\n",
    "- resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv5-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "## JSON横竖切换\n",
    "  - 快捷键Ctrl + Alt + L "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbc11d",
   "metadata": {},
   "source": [
    "### Dataset summary:\n",
    "    - Recognition: Imagenet(ILSVRC 2012), CIFAR-10\n",
    "    - Detect: COCO, Imagenet(ILSVRC 2012)\n",
    "    - Semantic Segmentation: Cityscapes, PASCAL VOC 2012, R-ASPP and Lite R-ASPP\n",
    "    - Super Resolution: DIV2k, Set5, Set14, BSD100, Urban100, Manga109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e39283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a820878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check models \n",
    "import torchvision.models as models\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e91db25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Cityscapes\n",
       "    Number of datapoints: 2975\n",
       "    Root location: /home/royliu/Documents/datasets/cityscapes\n",
       "    Split: train\n",
       "    Mode: gtFine\n",
       "    Type: ['semantic']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cityscapes dataset\n",
    "from torchvision.datasets import Cityscapes as cityscapes  # will unzip the file when run at first time\n",
    "import os\n",
    "root ='/home/royliu/Documents/datasets/'\n",
    "data_dir ='cityscapes'\n",
    "\n",
    "data_dir = os.path.join(root, data_dir)\n",
    "dataset = cityscapes(data_dir, split = 'train', mode ='fine', target_type = 'semantic')\n",
    "img, smnt = dataset[0]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS coco detection dataset\n",
    "CLASS torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'deeplabv3_resnet50':'Cityscapes', }\n",
    "\n",
    "{'deeplabv3_resnet50':'torchvision.models.segmentation.deeplabv3_resnet50'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://pytorch.org/vision/stable/models.html \n",
    "'''\n",
    "model_list_all = ['alexnet', 'convnext_base', 'densenet121','densenet201','efficientnet_v2_l', \\\n",
    "                  'googlenet','inception_v3','mnasnet0_5','mobilenet_v2','mobilenet_v3_small',\\\n",
    "                  'regnet_y_400mf','resnet18','resnet50', 'resnet152','shufflenet_v2_x1_0', \\\n",
    "                  'squeezenet1_0', 'squeezenet1_1','vgg11', 'vgg16', 'vgg19','vit_b_16']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1c3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"mobilenet_v2\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"vgg16\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# training\n",
    "img_size_list = [224]\n",
    "batch_size_list_train = [64,128,256,512,1024]\n",
    "train_config_list = config_list\n",
    "for image_size in img_size_list:\n",
    "    for model in train_config_list:\n",
    "        for batch_size in batch_size_list_train:\n",
    "            new_args = {'arch': model,'workers': 1, 'epochs': 3, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_train.append(new_args)\n",
    "\n",
    "# infering   \n",
    "img_size_list = [224]\n",
    "batch_size_list_infer = [1]\n",
    "infer_config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "for image_size in img_size_list:\n",
    "    for model in infer_config_list:\n",
    "        for batch_size in batch_size_list_infer:\n",
    "            new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_infer.append(new_args)\n",
    "        \n",
    "tr = json.dumps(args_train)\n",
    "inf= json.dumps(args_infer)\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a757e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "532020c6",
   "metadata": {},
   "source": [
    "# Experiments record:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667fbc5",
   "metadata": {},
   "source": [
    "\n",
    "### by 12_21_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [64, 128,256,512]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet','mobilenet_v2','resnet152', 'vgg16']\n",
    "\n",
    "\n",
    "### by 12_22_2022 :\n",
    "\n",
    "    - training\n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1024]\n",
    "            - config_list = ['alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "    - infering   \n",
    "            - img_size_list = [224]\n",
    "            - batch_size_list = [1]\n",
    "            - config_list = ['alexnet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c818f",
   "metadata": {},
   "source": [
    "##  01172023\n",
    "### training:\n",
    "  - deeplab_v3'\n",
    "       batch size: 2,4,6,8\n",
    "  - 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "       batch size: 64,128,256,512,1024\n",
    "\n",
    "### Inference:\n",
    " - yolov5\n",
    " - 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24d3b2",
   "metadata": {},
   "source": [
    "##  COMBINATION 01/20/2023 - 01/23a/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    - 'yolo_v5', 'alexnet','densenet201','mobilenet_v2','resnet152', 'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "    \n",
    "  #### except:\n",
    "    train:\n",
    "    - 'shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024,1\n",
    "    - image size 224\n",
    "    \n",
    "    infer:\n",
    "    'vgg16'\n",
    "    - batch size: 1\n",
    "    - image size: 224\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab169a",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ec7b0",
   "metadata": {},
   "source": [
    "## COMBINATION: 01/23/2023 - 01/24/2023\n",
    "### training:\n",
    "    - 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "    - batch size: 2,4,8,16,32,64,128,256,512,1024\n",
    "    - image size 224\n",
    "\n",
    "### Inference:\n",
    "    -  'yolo_v5s', 'alexnet','densenet201','mobilenet_v2'\n",
    "    - batch size: 1\n",
    "    - image size: 448\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1032d2",
   "metadata": {},
   "source": [
    "###  \\# 01/25/2023 - 01/26a/2023\n",
    "#### COMBINATION: \n",
    "#### training:\n",
    "- 'deeplab_v3','alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16'\n",
    "- batch size: 2,4,8,16,32,64,128,256,512,1024\n",
    "- image size 224\n",
    "\n",
    "#### Inference:\n",
    "-  'yolo_v5s', 'alexnet'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "###  \\# 01/27/2023 - 01/28/2023, for training duration comparing individually training and co-work with inference\n",
    "#### COMBINATION: \n",
    "#### Training: \n",
    "- 'deeplab_v3': batch size:  8\n",
    "- 'vgg16','resnet152': batch size: 64 and 32 individually\n",
    "- Epochs: 5, image size 224\n",
    "\n",
    "#### Inference: \n",
    "- 'None', 'yolo_v5s','mobilenet_v2'\n",
    "- batch size: 1\n",
    "- image size: 224\n",
    "\n",
    "#### result: canceled with exception (after runing 43992 seconds)\n",
    "##### Exception:\n",
    "Train arch_vgg16 workers_1 epochs_5 batch_size_64 image_size_224 device_cuda + Infer arch_None workers_1 batch_size_1 image_size_224 device_cuda\n",
    "{'data': '/home/royliu/Documents/datasets', 'dataset': 'imagenet', 'arch': 'vgg16', 'workers': 1, 'epochs': 5, 'start_epoch': 0, 'batch_size': 64, 'lr': 0.001, 'momentum': 0.9,\n",
    "\n",
    "100%|██████████| 20019/20019 [13:03:14<00:00,  2.35s/it]\n",
    " 43992  instances. *   Acc@1 0.918 Acc@5 3.806\n",
    " FileNotFoundError: [Errno 2] No such file or directory: 'checkpoint.pth.tar'\n",
    " \n",
    "### \\# 01/29/2023 - 01/30/2023, for training duration comparing individually training and co-work with inference\n",
    "#### COMBINATION: \n",
    "##### Training: \n",
    "- 'vgg16','resnet152': batch size: 64 and 32 individually\n",
    "- Epochs: 5, image size 224\n",
    "\n",
    "##### Inference: \n",
    "- 'yolo_v5s','None', 'mobilenet_v2'\n",
    "- batch size: 1\n",
    "- image size: 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd46fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7567140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"vgg16\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"resnet152\", \"workers\": 1, \"epochs\": 5, \"batch_size\": 64, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 20230118\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "config_list = ['vgg16', 'resnet152']\n",
    "\n",
    "\n",
    "# training\n",
    "img_size_list = [224]\n",
    "batch_size_list_train = [64] ## deeplab_v3 is  2, 4, 6, 8 instead\n",
    "train_config_list = config_list\n",
    "for image_size in img_size_list:\n",
    "    for model in train_config_list:\n",
    "        for batch_size in batch_size_list_train:\n",
    "            new_args = {'arch': model,'workers': 1, 'epochs': 5, 'batch_size': batch_size, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_train.append(new_args)\n",
    "\n",
    "tr = json.dumps(args_train)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e64ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"arch\": \"None\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"yolo_v5s\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}, {\"arch\": \"alexnet\", \"workers\": 1, \"batch_size\": 1, \"image_size\": 224, \"device\": \"cuda\"}]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 20230118\n",
    "import json\n",
    "args_train, args_infer = [] ,[]\n",
    "# config_list = ['yolo_v5', 'alexnet','densenet201','mobilenet_v2','resnet152','shufflenet_v2_x1_0', 'squeezenet1_0', 'vgg16']\n",
    "\n",
    "\n",
    "# infering   \n",
    "img_size_list = [224]\n",
    "batch_size_list_infer = [1]\n",
    "infer_config_list = ['None','yolo_v5s','alexnet']\n",
    "for image_size in img_size_list:\n",
    "    for model in infer_config_list:\n",
    "        for batch_size in batch_size_list_infer:\n",
    "            new_args = {'arch': model,'workers': 1, 'batch_size': 1, 'image_size':image_size, 'device': 'cuda'}\n",
    "            args_infer.append(new_args)\n",
    "            \n",
    "inf= json.dumps(args_infer)\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3820b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15258615",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7c6d5",
   "metadata": {},
   "source": [
    "#### models in torchvision.models:\n",
    " - inception_v3:\n",
    "    - RuntimeError: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
    "\n",
    " - googlenet:\n",
    "     - AttributeError: 'GoogLeNetOutputs' object has no attribute 'log_softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ad9a",
   "metadata": {},
   "source": [
    "## Summary of models and dataset in Mobile AI\n",
    " - in https://ai-benchmark.com/tests.html\n",
    "\n",
    "#### Recognition task:\n",
    "- MobileNet - V2, 224x224, Imagenet + MS COCO\n",
    "    train Imagenet, detection: MS COCO (with a modified version of the Single Shot Detector (SSD))\n",
    "- Inception -V3, 346x346, ILSVRC 2012, inference: recognition\n",
    "- MobileNet - V3,  512 x 512,\n",
    "    classification: Imagenet, MobilenetNet V3\n",
    "    detection: . detection: MS COCO (replacement for the backbone feature extractor in SSDLite)\n",
    "    Semantic Segmentation: Cityscapes, R-ASPP and Lite R-ASPP as head\n",
    "- EfficientNet-B4, 380 x 380 ,  CIFAR-10, subset of the CamSDD datase\n",
    "- Inception -V3, 346x346, NA\n",
    "\n",
    "\n",
    " - alexnet\n",
    " - vgg\n",
    " - resnet\n",
    "\n",
    "#### Object Detection:\n",
    " - YOLOv4-Tiny, 416 x 416,\n",
    "    dataset： train with imagenet, detect with: MS COCO\n",
    "\n",
    " - CRNN / Bi-LSTM , 64 x 200\n",
    "    train: synthetic dataset (Synth) released by Jaderberg et al.\n",
    "    test: y ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).\n",
    "\n",
    "#### Semantic Segmentation:\n",
    " -  DeepLab-V3+ , 1024 x 1024,\n",
    "    dataset:\n",
    "        employ ImageNet-1k pretrained ResNet-101 or modified aligned Xception to extract dense feature maps\n",
    "        test: PASCAL VOC 2012 and Cityscapes datasets\n",
    "            backbone: {ResNet-101:513 x 513, xception: 299 x 299}\n",
    "\n",
    "###### Super resolution:\n",
    " - ESRGAN, 512 x512,\n",
    "    dataset:\n",
    "        train: DIV2K dataset (PIRM2018-SR Challenge), Flickr2K,  OutdoorSceneTraining (OST)\n",
    "        evaluate: Set5 , Set14, BSD100, Urban100\n",
    " - SRGAN, 1024 x 1024\n",
    "    dataset:\n",
    "        train: a random sample of 350 thousand images from the ImageNet database\n",
    "        evaluate: Set5 , Set14 and BSD100\n",
    "\n",
    "###### Video Super-Resolution:\n",
    " - XLSR, 1080 x 1920\n",
    "    dataset:\n",
    "        train:Div2K\n",
    "        evaluate: Div2K 100 test images,  Set5, Set14, BSD100, Manga109, Urban100\n",
    " - VSR, 2160 x 3840, dataset:  {train and test: DIV2K}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3621fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'config.json'\n",
    "with open(file) as f:\n",
    "    config = json.load(f)\n",
    "args_train, args_infer= config['train'],  config['infer']\n",
    "len(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973aeb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'122415'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def date_time():\n",
    "    s_l = time.localtime(time.time())\n",
    "    date = time.strftime(\"%Y%m%d\", s_l)\n",
    "    tm = time.strftime(\"%H%M%S\", s_l)\n",
    "    # print(date, tm )\n",
    "    return date, tm\n",
    "\n",
    "date, tm = date_time()\n",
    "tm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
